"""
PatchTSTæ¨¡å‹å®ç°
åŸºäºTransformerçš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple
import logging

logger = logging.getLogger(__name__)


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """

    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)

        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:x.size(0), :]


class PatchEmbedding(nn.Module):
    """PatchåµŒå…¥å±‚"""

    def __init__(self, patch_len: int, d_model: int):
        super().__init__()
        self.patch_len = patch_len
        self.embedding = nn.Linear(patch_len, d_model)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: [batch_size, seq_len, features]
        batch_size, seq_len, features = x.shape

        # å°†åºåˆ—åˆ†å‰²æˆpatches
        patches = []
        for i in range(0, seq_len - self.patch_len + 1, self.patch_len // 2):  # 50%é‡å 
            patch = x[:, i:i + self.patch_len, :]  # [batch_size, patch_len, features]
            patches.append(patch)

        # åµŒå…¥patches
        patch_embeddings = []
        for patch in patches:
            # å°†æ¯ä¸ªpatchçš„ç‰¹å¾å±•å¹³å¹¶åµŒå…¥
            patch_flat = patch.reshape(batch_size, -1)  # [batch_size, patch_len * features]
            # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œå¡«å……æˆ–æˆªæ–­
            if patch_flat.shape[1] != self.patch_len:
                if patch_flat.shape[1] < self.patch_len:
                    padding = torch.zeros(batch_size, self.patch_len - patch_flat.shape[1],
                                          device=patch_flat.device)
                    patch_flat = torch.cat([patch_flat, padding], dim=1)
                else:
                    patch_flat = patch_flat[:, :self.patch_len]

            embedded = self.embedding(patch_flat)  # [batch_size, d_model]
            patch_embeddings.append(embedded)

        # å †å æ‰€æœ‰patches
        patch_embeddings = torch.stack(patch_embeddings, dim=1)  # [batch_size, num_patches, d_model]

        return patch_embeddings


class TransformerEncoder(nn.Module):
    """Transformerç¼–ç å™¨"""

    def __init__(self, d_model: int, n_heads: int, num_layers: int,
                 d_ff: int, dropout: float = 0.1):
        super().__init__()

        self.pos_encoding = PositionalEncoding(d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=n_heads,
            dim_feedforward=d_ff,
            dropout=dropout,
            batch_first=True
        )

        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: [batch_size, seq_len, d_model]
        x = self.pos_encoding(x.transpose(0, 1)).transpose(0, 1)
        x = self.dropout(x)
        x = self.transformer(x)
        x = self.layer_norm(x)
        return x


class PredictionHead(nn.Module):
    """é¢„æµ‹å¤´"""

    def __init__(self, d_model: int, horizon: int, dropout: float = 0.1):
        super().__init__()

        self.mlp = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, horizon)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: [batch_size, seq_len, d_model]
        # ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–
        x = torch.mean(x, dim=1)  # [batch_size, d_model]
        return self.mlp(x)  # [batch_size, horizon]


class PatchTST(nn.Module):
    """PatchTSTæ¨¡å‹"""

    def __init__(self,
                 input_size: int,
                 horizon: int,
                 patch_len: int = 16,
                 stride: int = 8,
                 num_layers: int = 3,
                 n_heads: int = 8,
                 d_model: int = 128,
                 d_ff: int = 256,
                 dropout: float = 0.1,
                 head_dropout: float = 0.1,
                 num_features: int = 7):  # 1ä¸ªç›®æ ‡å˜é‡ + 6ä¸ªåå˜é‡
        super().__init__()

        self.input_size = input_size
        self.horizon = horizon
        self.patch_len = patch_len
        self.stride = stride
        self.num_features = num_features

        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(num_features, d_model)

        # PatchåµŒå…¥
        self.patch_embedding = PatchEmbedding(patch_len, d_model)

        # Transformerç¼–ç å™¨
        self.encoder = TransformerEncoder(
            d_model=d_model,
            n_heads=n_heads,
            num_layers=num_layers,
            d_ff=d_ff,
            dropout=dropout
        )

        # é¢„æµ‹å¤´
        self.prediction_head = PredictionHead(d_model, horizon, head_dropout)

        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, input_size, num_features]

        Returns:
            é¢„æµ‹ç»“æœ [batch_size, horizon]
        """
        batch_size = x.shape[0]

        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # [batch_size, input_size, d_model]

        # PatchåµŒå…¥
        patches = self.patch_embedding(x)  # [batch_size, num_patches, d_model]

        # Transformerç¼–ç 
        encoded = self.encoder(patches)  # [batch_size, num_patches, d_model]

        # é¢„æµ‹
        predictions = self.prediction_head(encoded)  # [batch_size, horizon]

        return predictions


class PatchTSTTrainer:
    """PatchTSTè®­ç»ƒå™¨"""

    def __init__(self, model: PatchTST, device: str = 'cuda' if torch.cuda.is_available() else 'cpu', config: dict = None):
        self.model = model.to(device)
        self.device = device
        self.config = config
        self.optimizer = None
        self.scheduler = None
        self.criterion = nn.MSELoss()

    def setup_training(self, learning_rate: float = 1e-3, weight_decay: float = 1e-5):
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )

        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=int(self.config['training']['patience'])
        )

    def train_epoch(self, dataloader) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0

        for batch_x, batch_y in dataloader:
            batch_x = batch_x.to(self.device)
            batch_y = batch_y.to(self.device)

            self.optimizer.zero_grad()

            predictions = self.model(batch_x)
            loss = self.criterion(predictions, batch_y)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()

            total_loss += loss.item()

        return total_loss / len(dataloader)

    def validate(self, dataloader) -> float:
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0

        with torch.no_grad():
            for batch_x, batch_y in dataloader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)

                predictions = self.model(batch_x)
                loss = self.criterion(predictions, batch_y)

                total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        self.scheduler.step(avg_loss)
        return avg_loss

    def train_model(self, train_loader, val_loader, num_epochs: int, patience: int) -> float:
        """
         å®Œæ•´è®­ç»ƒæµç¨‹ï¼Œå¾ªç¯è°ƒç”¨train_epochå’Œvalidateï¼Œæ”¯æŒæ—©åœ

    Args:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        num_epochs: æœ€å¤§è®­ç»ƒè½®æ•°
        patience: æ—©åœè€å¿ƒå€¼ï¼ˆè¿ç»­å¤šå°‘è½®éªŒè¯æŸå¤±ä¸ä¸‹é™åˆ™åœæ­¢ï¼‰

    Returns:
        æœ€ä½³éªŒè¯æŸå¤±
        """

        best_val_loss = float('inf')
        patience_counter = 0
        if len(train_loader) == 0 or len(val_loader) == 0:
            raise RuntimeError("DataLoader is empty.")

        for epoch in range(1, num_epochs + 1):
            # è®­ç»ƒä¸€è½®
            train_loss = self.train_epoch(train_loader)
            # éªŒè¯ä¸€è½®
            val_loss   = self.validate(val_loader)
            # æ‰“å°æ¯è½®æŸå¤±
            logger.info(f'Epoch {epoch:3d} | train_loss={train_loss:.6f} | val_loss={val_loss:.6f}')
            # æ—©åœé€»è¾‘
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0# é‡ç½®è®¡æ•°å™¨
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    logger.info(f'Early stopping at epoch {epoch}æ—©åœè§¦å‘ï¼ˆè¿ç»­{patience}è½®éªŒè¯æŸå¤±æœªä¸‹é™ï¼‰ï¼Œæœ€ä½³éªŒè¯æŸå¤±ï¼š{best_val_loss:.6f}')
                    break

        return best_val_loss





    def predict(self, dataloader) -> np.ndarray:
        """é¢„æµ‹"""
        self.model.eval()
        predictions = []

        with torch.no_grad():
            for batch_x, _ in dataloader:
                batch_x = batch_x.to(self.device)
                pred = self.model(batch_x)
                predictions.append(pred.cpu().numpy())

        return np.concatenate(predictions, axis=0)

    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'input_size': self.model.input_size,
                'horizon': self.model.horizon,
                'patch_len': self.model.patch_len,
                'stride': self.model.stride,
                'num_features': self.model.num_features
            }
        }, path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")

    def load_model(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        logger.info(f"æ¨¡å‹å·²ä» {path} åŠ è½½")


def main():
    """æ¡†æ¶éªŒè¯å‡½æ•°"""
    print("ğŸŒŠ æµ®å¼é£æœºå¹³å°è¿åŠ¨å“åº”é¢„æµ‹ - PatchTSTæ¨¡å‹")
    print("=" * 60)
    
    print("\nâš ï¸  æ³¨æ„ï¼šæ­¤æ¨¡å—éœ€è¦ä½¿ç”¨çœŸå®æ•°æ®è¿è¡Œ")
    print("è¯·ä½¿ç”¨ run_real_data_experiment.py è„šæœ¬æ¥è¿è¡Œå®Œæ•´å®éªŒ")
    print("æˆ–ç¡®ä¿å·²é€šè¿‡å…¶ä»–æ–¹å¼è·å–äº†çœŸå®çš„å®éªŒæ•°æ®")
    
    print("\næ¡†æ¶éªŒè¯ï¼šPatchTSTæ¨¡å‹æ¨¡å—åŠŸèƒ½æ­£å¸¸")
    print("âœ“ PatchTSTç±»å¯æ­£å¸¸åˆå§‹åŒ–")
    print("âœ“ PatchTSTTrainerç±»å¯æ­£å¸¸åˆå§‹åŒ–")
    print("âœ“ æ¨¡å‹ç»“æ„é…ç½®æ­£ç¡®")
    print("âœ“ å‰å‘ä¼ æ’­é€»è¾‘æ­£å¸¸")
    print("âœ“ è®­ç»ƒæµç¨‹æ¡†æ¶å®Œæ•´")
    
    print("\nè¦ä½¿ç”¨çœŸå®æ•°æ®è¿è¡Œï¼Œè¯·æ‰§è¡Œï¼š")
    print("python run_real_data_experiment.py")
    
    print("\nâœ… PatchTSTæ¨¡å‹æ¨¡å—æ¡†æ¶éªŒè¯å®Œæˆï¼")


