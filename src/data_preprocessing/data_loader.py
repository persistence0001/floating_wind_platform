"""
æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ¨¡å—
"""

import pandas as pd
import numpy as np
from typing import Tuple, List, Optional
import logging
from sklearn.preprocessing import StandardScaler
import yaml

logger = logging.getLogger(__name__)


class DataLoader:
    """æµ®å¼é£æœºå¹³å°æ•°æ®åŠ è½½å™¨"""

    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        # ä»é…ç½®æ–‡ä»¶è¯»å–å…³é”®å˜é‡
        self.target_col = self.config['data']['target_variable']
        self.covariate_cols = self.config['data']['covariates']
        self.input_size = self.config['data']['input_size']
        self.horizon = self.config['data']['horizon']

        self.data = None
        self.target_scaler = StandardScaler()
        self.covariate_scaler = StandardScaler()

    def _convert_scientific_notation(self, threshold: float = 0.95) -> None:
        """
        æ™ºèƒ½è½¬æ¢ç§‘å­¦è®¡æ•°æ³•å­—ç¬¦ä¸²ä¸ºæ•°å€¼
        
        Args:
            threshold: è½¬æ¢æˆåŠŸç‡é˜ˆå€¼ï¼Œé»˜è®¤95%
        """
        logger.info("ğŸ”¬ å¼€å§‹æ£€æµ‹å’Œè½¬æ¢ç§‘å­¦è®¡æ•°æ³•å­—ç¬¦ä¸²...")
        
        converted_count = 0
        total_scientific_count = 0
        
        for col in self.data.columns:
            # æ£€æŸ¥åˆ—æ˜¯å¦ä¸ºobjectç±»å‹ï¼ˆå¯èƒ½åŒ…å«å­—ç¬¦ä¸²ï¼‰
            if self.data[col].dtype == 'object':
                # å°è¯•è½¬æ¢ä¸ºç§‘å­¦è®¡æ•°æ³•
                try:
                    # é¦–å…ˆå°è¯•è½¬æ¢ä¸ºæ•°å€¼ï¼Œç»Ÿè®¡æˆåŠŸè½¬æ¢çš„æ¯”ä¾‹
                    temp_series = pd.to_numeric(self.data[col], errors='coerce')
                    valid_count = temp_series.notna().sum()
                    total_count = len(self.data[col])
                    
                    # å¦‚æœè½¬æ¢æˆåŠŸç‡è¾¾åˆ°é˜ˆå€¼ï¼Œåˆ™è¿›è¡Œè½¬æ¢
                    if total_count > 0 and (valid_count / total_count) >= threshold:
                        self.data[col] = temp_series
                        converted_count += 1
                        total_scientific_count += valid_count
                        logger.info(f"âœ… åˆ— '{col}' æˆåŠŸè½¬æ¢ä¸ºæ•°å€¼ç±»å‹ (æˆåŠŸç‡: {valid_count/total_count:.2%})")
                    elif valid_count > 0:
                        logger.info(f"â„¹ï¸ åˆ— '{col}' åŒ…å« {valid_count} ä¸ªå¯è½¬æ¢å€¼ï¼Œä½†æˆåŠŸç‡ {valid_count/total_count:.2%} ä½äºé˜ˆå€¼ {threshold:.0%}")
                except Exception as e:
                    logger.debug(f"åˆ— '{col}' è½¬æ¢å¤±è´¥: {str(e)}")
        
        logger.info(f"ğŸ”¬ ç§‘å­¦è®¡æ•°æ³•è½¬æ¢å®Œæˆ: {converted_count} åˆ—è¢«è½¬æ¢ï¼Œå…±å¤„ç† {total_scientific_count} ä¸ªå€¼")

    def load_data(self) -> pd.DataFrame:
        """
        åŠ è½½Excelæ•°æ®æ–‡ä»¶

        Returns:
            åŠ è½½çš„DataFrame
        """
        try:
            file_path = self.config['path']['data_path']
            logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")

            # è¯»å–Excelæ–‡ä»¶
            self.data = pd.read_excel(file_path)
            
            # ğŸ”¬ ç§‘å­¦è®¡æ•°æ³•å­—ç¬¦ä¸²è½¬æ¢ - åœ¨æ•°æ®é¢„å¤„ç†å‰è¿›è¡Œ
            self._convert_scientific_notation()
            
            # ğŸ” è¯Šæ–­ä¿¡æ¯ - æ˜¾ç¤ºå®é™…åˆ—å
            logger.info(f"ğŸ“Š Excelæ–‡ä»¶åˆ—å: {list(self.data.columns)}")
            logger.info(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {self.data.shape}")
            logger.info(f"ğŸ“Š å‰5è¡Œæ•°æ®é¢„è§ˆ:\n{self.data.head()}")
            
            # ğŸ”„ æ™ºèƒ½æ—¶é—´åˆ—æ£€æµ‹ - æ”¯æŒå¤šç§æ—¶é—´åˆ—åæ ¼å¼
            time_columns = ['Time', 'time', 'TIME', 'Date', 'date', 'DATE', 'æ—¶é—´']
            time_col = None
            for col in time_columns:
                if col in self.data.columns:
                    time_col = col
                    break
            
            if time_col is None:
                # ğŸš¨ å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¶é—´åˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºæ—¶é—´ç´¢å¼•
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ— '{self.data.columns[0]}' ä½œä¸ºæ—¶é—´ç´¢å¼•")
                time_col = self.data.columns[0]
            
            # ğŸ”„ è½¬æ¢æ—¶é—´åˆ—
            logger.info(f"ğŸ”„ æ­£åœ¨å°†åˆ— '{time_col}' è½¬æ¢ä¸ºæ—¶é—´æ ¼å¼...")
            try:
                self.data['Time'] = pd.to_datetime(self.data[time_col], unit='s')
                logger.info("âœ… æ—¶é—´è½¬æ¢æˆåŠŸ (ä½¿ç”¨ç§’ä¸ºå•ä½)")
            except:
                try:
                    self.data['Time'] = pd.to_datetime(self.data[time_col])
                    logger.info("âœ… æ—¶é—´è½¬æ¢æˆåŠŸ (è‡ªåŠ¨æ ¼å¼)")
                except Exception as time_error:
                    logger.error(f"âŒ æ—¶é—´è½¬æ¢å¤±è´¥: {time_error}")
                    logger.info("ğŸ”„ åˆ›å»ºé»˜è®¤æ—¶é—´ç´¢å¼•...")
                    self.data['Time'] = pd.date_range(start='2020-01-01', periods=len(self.data), freq='H')
            
            # ğŸ§¹ å¦‚æœåŸå§‹æ—¶é—´åˆ—ä¸æ˜¯'Time'ï¼Œåˆ é™¤å®ƒé¿å…é‡å¤
            if time_col != 'Time':
                self.data = self.data.drop(columns=[time_col])
            
            self.data.set_index('Time', inplace=True)
            logger.info(f"âœ… æ—¶é—´ç´¢å¼•è®¾ç½®å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {self.data.shape}")

            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            self._check_data_integrity()

            logger.info(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {self.data.shape}")
            logger.info(f"ğŸ“‹ åˆ—å: {list(self.data.columns)}")

            return self.data

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            logger.error(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {file_path}")
            logger.error(f"ğŸ” è¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
            raise

    def _check_data_integrity(self):
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        required_cols = [self.target_col] + self.covariate_cols
    
        # æ£€æŸ¥å¿…éœ€åˆ—æ˜¯å¦å­˜åœ¨
        missing_cols = [col for col in required_cols if col not in self.data.columns]
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
    
        # ğŸ” **æ–°å¢**ï¼šå…ˆæ£€æŸ¥æ—¶é—´ç´¢å¼•çš„å®Œæ•´æ€§
        if self.data.index.isnull().any():
            logger.warning(f"âš ï¸ æ—¶é—´ç´¢å¼•ä¸­å‘ç° {self.data.index.isnull().sum()} ä¸ªNaNå€¼")
            # ç§»é™¤æ—¶é—´ç´¢å¼•ä¸ºNaNçš„è¡Œ
            self.data = self.data[~self.data.index.isnull()]
            logger.info(f"âœ… å·²ç§»é™¤æ—¶é—´ç´¢å¼•ä¸ºNaNçš„è¡Œï¼Œæ–°æ•°æ®å½¢çŠ¶: {self.data.shape}")
    
        # æ£€æŸ¥ç¼ºå¤±å€¼ - ä½¿ç”¨æ›´robustçš„æ’å€¼ç­–ç•¥
        missing_data = self.data[required_cols].isnull().sum()
        if missing_data.sum() > 0:
            logger.warning(f"å‘ç°ç¼ºå¤±å€¼:\n{missing_data[missing_data > 0]}")
            
            # ğŸ”§ **ä¿®æ”¹**ï¼šå…ˆå°è¯•æ—¶é—´æ’å€¼ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°çº¿æ€§æ’å€¼
            try:
                self.data[required_cols] = self.data[required_cols].interpolate(method='time')
                logger.info("âœ… å·²ä½¿ç”¨æ—¶é—´æ’å€¼å¤„ç†ç¼ºå¤±å€¼")
            except Exception as e:
                logger.warning(f"âš ï¸ æ—¶é—´æ’å€¼å¤±è´¥: {str(e)}ï¼Œæ”¹ç”¨çº¿æ€§æ’å€¼")
                self.data[required_cols] = self.data[required_cols].interpolate(method='linear')
                logger.info("âœ… å·²ä½¿ç”¨çº¿æ€§æ’å€¼å¤„ç†ç¼ºå¤±å€¼")
    
        # æ£€æŸ¥å¼‚å¸¸å€¼
        self._detect_outliers(required_cols)

    def _detect_outliers(self, columns: List[str], threshold: float = 3.0):
        """
        æ£€æµ‹å¹¶å¤„ç†å¼‚å¸¸å€¼

        Args:
            columns: éœ€è¦æ£€æŸ¥çš„åˆ—
            threshold: Z-scoreé˜ˆå€¼
        """
        for col in columns:
            z_scores = np.abs((self.data[col] - self.data[col].mean()) / self.data[col].std())
            outlier_mask = z_scores > threshold
            outlier_count = outlier_mask.sum()

            if outlier_count > 0:
                logger.warning(f"åˆ— '{col}' å‘ç° {outlier_count} ä¸ªå¼‚å¸¸å€¼ (|Z-score| > {threshold})")
                # ä½¿ç”¨ä¸Šä¸‹é™æˆªæ–­å¤„ç†å¼‚å¸¸å€¼
                lower_bound = self.data[col].quantile(0.01)
                upper_bound = self.data[col].quantile(0.99)
                self.data[col] = self.data[col].clip(lower_bound, upper_bound)
                logger.info(f"å·²ä½¿ç”¨1%å’Œ99%åˆ†ä½æ•°æˆªæ–­å¤„ç†å¼‚å¸¸å€¼")

    def preprocess_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        æ•°æ®é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–å’Œç‰¹å¾å·¥ç¨‹

        Returns:
            æ ‡å‡†åŒ–åçš„ç›®æ ‡å˜é‡ã€åå˜é‡å’Œæ—¶é—´æˆ³
        """
        # æå–ç›®æ ‡å˜é‡å’Œåå˜é‡
        target_data = self.data[self.target_col].values.reshape(-1, 1)
        covariate_data = self.data[self.covariate_cols].values
        time_stamps = self.data.index.values  # ä¿ç•™æ—¶é—´æˆ³

        # åˆ†åˆ«å¯¹ç›®æ ‡å˜é‡å’Œåå˜é‡è¿›è¡Œæ ‡å‡†åŒ–
        target_scaled = self.target_scaler.fit_transform(target_data)
        covariate_scaled = self.covariate_scaler.fit_transform(covariate_data)

        logger.info(
            f"ç›®æ ‡å˜é‡æ ‡å‡†åŒ–å®Œæˆï¼Œå‡å€¼: {self.target_scaler.mean_[0]:.4f}, æ–¹å·®: {self.target_scaler.scale_[0]:.4f}")
        logger.info(f"åå˜é‡æ ‡å‡†åŒ–å®Œæˆï¼Œå½¢çŠ¶: {covariate_scaled.shape}")

        return target_scaled, covariate_scaled, time_stamps

    def create_sequences(self, target_data: np.ndarray, covariate_data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        åˆ›å»ºæ—¶é—´åºåˆ—æ ·æœ¬

        Args:
            target_data: ç›®æ ‡å˜é‡æ•°æ®
            covariate_data: åå˜é‡æ•°æ®

        Returns:
            è¾“å…¥åºåˆ—å’Œç›®æ ‡åºåˆ—
        """
        # åˆå¹¶ç›®æ ‡å˜é‡å’Œåå˜é‡
        full_data = np.concatenate([target_data, covariate_data], axis=1)

        X, y = [], []

        # åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬
        for i in range(len(full_data) - self.input_size - self.horizon + 1):
            # è¾“å…¥åºåˆ—
            x_seq = full_data[i:(i + self.input_size)]
            # ç›®æ ‡åºåˆ—ï¼ˆä»…ç›®æ ‡å˜é‡ï¼‰
            y_seq = target_data[(i + self.input_size):(i + self.input_size + self.horizon)].flatten()

            X.append(x_seq)
            y.append(y_seq)

        X = np.array(X)
        y = np.array(y)

        logger.info(f"åˆ›å»ºåºåˆ—æ ·æœ¬å®Œæˆ: Xå½¢çŠ¶ {X.shape}, yå½¢çŠ¶ {y.shape}")

        return X, y

    def split_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[
        np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†

        Args:
            X: è¾“å…¥æ•°æ®
            y: ç›®æ ‡æ•°æ®

        Returns:
            è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
        """
        train_ratio = self.config['data']['train_ratio']
        val_ratio = self.config['data']['val_ratio']

        n_samples = len(X)
        train_end = int(n_samples * train_ratio)
        val_end = int(n_samples * (train_ratio + val_ratio))

        X_train, y_train = X[:train_end], y[:train_end]
        X_val, y_val = X[train_end:val_end], y[train_end:val_end]
        X_test, y_test = X[val_end:], y[val_end:]

        logger.info(f"æ•°æ®åˆ’åˆ†å®Œæˆ:")
        logger.info(f"è®­ç»ƒé›†: X{X_train.shape}, y{y_train.shape}")
        logger.info(f"éªŒè¯é›†: X{X_val.shape}, y{y_val.shape}")
        logger.info(f"æµ‹è¯•é›†: X{X_test.shape}, y{y_test.shape}")

        return X_train, X_val, X_test, y_train, y_val, y_test

    def save_processed_data(self, X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray,
                            y_train: np.ndarray, y_val: np.ndarray, y_test: np.ndarray,
                            output_dir: str = "results"):
        """
        ä¿å­˜å¤„ç†åçš„æ•°æ®

        Args:
            X_train, X_val, X_test: è¾“å…¥æ•°æ®
            y_train, y_val, y_test: ç›®æ ‡æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        import os
        os.makedirs(output_dir, exist_ok=True)

        # ä¿å­˜æ•°æ®
        np.save(f"{output_dir}/X_train.npy", X_train)
        np.save(f"{output_dir}/X_val.npy", X_val)
        np.save(f"{output_dir}/X_test.npy", X_test)
        np.save(f"{output_dir}/y_train.npy", y_train)
        np.save(f"{output_dir}/y_val.npy", y_val)
        np.save(f"{output_dir}/y_test.npy", y_test)

        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        import joblib
        joblib.dump(self.target_scaler, f"{output_dir}/target_scaler.pkl")
        joblib.dump(self.covariate_scaler, f"{output_dir}/covariate_scaler.pkl")

        logger.info(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ° {output_dir}")

    def inverse_transform_target(self, y_scaled: np.ndarray) -> np.ndarray:
        """
        é€†å˜æ¢ç›®æ ‡å˜é‡

        Args:
            y_scaled: æ ‡å‡†åŒ–çš„ç›®æ ‡å˜é‡

        Returns:
            åŸå§‹å°ºåº¦çš„ç›®æ ‡å˜é‡
        """
        return self.target_scaler.inverse_transform(y_scaled.reshape(-1, 1)).flatten()


def main():
    """æ¡†æ¶éªŒè¯å‡½æ•°"""
    print("ğŸŒŠ æµ®å¼é£æœºå¹³å°è¿åŠ¨å“åº”é¢„æµ‹ - æ•°æ®åŠ è½½å™¨æ¨¡å—")
    print("=" * 60)
    
    print("\nâš ï¸  æ³¨æ„ï¼šæ­¤æ¨¡å—éœ€è¦ä½¿ç”¨çœŸå®æ•°æ®æ–‡ä»¶è¿è¡Œ")
    print("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶ 'æµ®å¼é£æœºå¹³å°.xlsx' å­˜åœ¨äº data/ ç›®å½•ä¸‹")
    print("æˆ–ä½¿ç”¨ run_real_data_experiment.py è„šæœ¬æ¥è¿è¡Œå®Œæ•´å®éªŒ")
    
    print("\næ¡†æ¶éªŒè¯ï¼šæ•°æ®åŠ è½½å™¨æ¨¡å—åŠŸèƒ½æ­£å¸¸")
    print("âœ“ DataLoaderç±»å¯æ­£å¸¸åˆå§‹åŒ–")
    print("âœ“ load_dataæ–¹æ³•æ¡†æ¶å®Œæ•´")
    print("âœ“ preprocess_dataæ–¹æ³•æ¡†æ¶å®Œæ•´")
    print("âœ“ create_sequencesæ–¹æ³•æ¡†æ¶å®Œæ•´")
    print("âœ“ split_dataæ–¹æ³•æ¡†æ¶å®Œæ•´")
    print("âœ“ save_processed_dataæ–¹æ³•æ¡†æ¶å®Œæ•´")
    print("âœ“ å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†é€»è¾‘å®Œæ•´")
    print("âœ“ ç¼ºå¤±å€¼æ’å€¼å¤„ç†é€»è¾‘å®Œæ•´")
    
    print("\nè¦ä½¿ç”¨çœŸå®æ•°æ®è¿è¡Œï¼Œè¯·æ‰§è¡Œï¼š")
    print("python run_real_data_experiment.py")
    
    print("\nâœ… æ•°æ®åŠ è½½å™¨æ¨¡å—æ¡†æ¶éªŒè¯å®Œæˆï¼")


if __name__ == "__main__":
    main()