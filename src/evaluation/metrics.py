"""
è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—
"""

import numpy as np
from typing import Dict, Tuple, Optional
import logging
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

logger = logging.getLogger(__name__)


class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""

    @staticmethod
    def calculate_mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """è®¡ç®—å¹³å‡ç»å¯¹è¯¯å·® (MAE)"""
        return mean_absolute_error(y_true.flatten(), y_pred.flatten())

    @staticmethod
    def calculate_mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """è®¡ç®—å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE)"""
        # é¿å…é™¤é›¶é”™è¯¯
        mask = np.abs(y_true) > 1e-10
        if np.sum(mask) == 0:
            return np.inf

        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        return mape

    @staticmethod
    def calculate_rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """è®¡ç®—å‡æ–¹æ ¹è¯¯å·® (RMSE)"""
        return np.sqrt(mean_squared_error(y_true.flatten(), y_pred.flatten()))

    @staticmethod
    def calculate_r2(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """è®¡ç®—å†³å®šç³»æ•° (RÂ²)"""
        return r2_score(y_true.flatten(), y_pred.flatten())

    @staticmethod
    def calculate_peak_error(y_true: np.ndarray, y_pred: np.ndarray, peak_percentage: float = 0.05) -> Dict:
        """
        è®¡ç®—å‰ç™¾åˆ†ä¹‹å‡ æœ€å¤§æ³¢å³°çš„é¢„æµ‹è¯¯å·®

        Args:
            y_true: çœŸå®å€¼ [n_samples, horizon]
            y_pred: é¢„æµ‹å€¼ [n_samples, horizon]
            peak_percentage: å³°å€¼ç™¾åˆ†æ¯” (0.05 = å‰5%)

        Returns:
            å³°å€¼è¯¯å·®ç»Ÿè®¡
        """
        n_samples, horizon = y_true.shape

        # åˆå¹¶æ‰€æœ‰æ—¶é—´æ­¥
        y_true_flat = y_true.flatten()
        y_pred_flat = y_pred.flatten()

        # æ‰¾åˆ°å‰ç™¾åˆ†ä¹‹å‡ çš„æœ€å¤§æ³¢å³°
        n_peaks = int(len(y_true_flat) * peak_percentage)

        # æ‰¾åˆ°çœŸå®å€¼ä¸­çš„å³°å€¼ç´¢å¼•
        peak_indices = np.argpartition(y_true_flat, -n_peaks)[-n_peaks:]

        # è®¡ç®—å³°å€¼è¯¯å·®
        peak_true = y_true_flat[peak_indices]
        peak_pred = y_pred_flat[peak_indices]

        peak_mae = np.mean(np.abs(peak_true - peak_pred))
        peak_rmse = np.sqrt(np.mean((peak_true - peak_pred) ** 2))
        peak_mape = np.mean(np.abs((peak_true - peak_pred) / peak_true)) * 100

        return {
            'peak_mae': peak_mae,
            'peak_rmse': peak_rmse,
            'peak_mape': peak_mape,
            'n_peaks': n_peaks,
            'peak_indices': peak_indices,
            'peak_true_values': peak_true,
            'peak_pred_values': peak_pred
        }

    @staticmethod
    def calculate_all_metrics(y_true: np.ndarray, y_pred: np.ndarray, peak_percentage: float = 0.05) -> Dict:
        """
        è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡

        Args:
            y_true: çœŸå®å€¼ [n_samples, horizon]
            y_pred: é¢„æµ‹å€¼ [n_samples, horizon]
            peak_percentage: å³°å€¼ç™¾åˆ†æ¯”

        Returns:
            æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        """
        metrics = {
            'MAE': EvaluationMetrics.calculate_mae(y_true, y_pred),
            'MAPE': EvaluationMetrics.calculate_mape(y_true, y_pred),
            'RMSE': EvaluationMetrics.calculate_rmse(y_true, y_pred),
            'R2': EvaluationMetrics.calculate_r2(y_true, y_pred)
        }

        # è®¡ç®—å³°å€¼è¯¯å·®
        peak_metrics = EvaluationMetrics.calculate_peak_error(y_true, y_pred, peak_percentage)
        metrics.update(peak_metrics)

        return metrics

    @staticmethod
    def calculate_directional_accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """
        è®¡ç®—æ–¹å‘å‡†ç¡®ç‡ï¼ˆé¢„æµ‹å˜åŒ–æ–¹å‘çš„å‡†ç¡®æ€§ï¼‰

        Args:
            y_true: çœŸå®å€¼ [n_samples, horizon]
            y_pred: é¢„æµ‹å€¼ [n_samples, horizon]

        Returns:
            æ–¹å‘å‡†ç¡®ç‡ (0-1)
        """
        # è®¡ç®—ç›¸é‚»æ—¶é—´æ­¥çš„å˜åŒ–
        true_changes = np.diff(y_true, axis=1)
        pred_changes = np.diff(y_pred, axis=1)

        # è®¡ç®—æ–¹å‘ä¸€è‡´æ€§
        same_direction = np.sign(true_changes) == np.sign(pred_changes)
        directional_accuracy = np.mean(same_direction)

        return directional_accuracy

    @staticmethod
    def calculate_timeliness_error(y_true: np.ndarray, y_pred: np.ndarray, threshold: float = 0.1) -> Dict:
        """
        è®¡ç®—æ—¶é—´è¯¯å·®ï¼ˆå³°å€¼é¢„æµ‹çš„æ—¶é—´åç§»ï¼‰

        Args:
            y_true: çœŸå®å€¼ [n_samples, horizon]
            y_pred: é¢„æµ‹å€¼ [n_samples, horizon]
            threshold: å³°å€¼é˜ˆå€¼

        Returns:
            æ—¶é—´è¯¯å·®ç»Ÿè®¡
        """
        n_samples, horizon = y_true.shape

        time_errors = []
        peak_detection_accuracy = []

        for i in range(n_samples):
            true_series = y_true[i]
            pred_series = y_pred[i]

            # æ£€æµ‹å³°å€¼ï¼ˆè¶…è¿‡å‡å€¼+é˜ˆå€¼*æ ‡å‡†å·®ï¼‰
            true_threshold = np.mean(true_series) + threshold * np.std(true_series)
            pred_threshold = np.mean(pred_series) + threshold * np.std(pred_series)

            true_peaks = np.where(true_series > true_threshold)[0]
            pred_peaks = np.where(pred_series > pred_threshold)[0]

            if len(true_peaks) > 0 and len(pred_peaks) > 0:
                # è®¡ç®—æ—¶é—´è¯¯å·®
                for true_peak in true_peaks:
                    if len(pred_peaks) > 0:
                        # æ‰¾åˆ°æœ€è¿‘çš„é¢„æµ‹å³°å€¼
                        closest_pred_idx = np.argmin(np.abs(pred_peaks - true_peak))
                        time_error = pred_peaks[closest_pred_idx] - true_peak
                        time_errors.append(time_error)

                # å³°å€¼æ£€æµ‹å‡†ç¡®ç‡
                true_positives = len(set(true_peaks) & set(pred_peaks))
                false_positives = len(set(pred_peaks) - set(true_peaks))
                false_negatives = len(set(true_peaks) - set(pred_peaks))

                if (true_positives + false_positives + false_negatives) > 0:
                    accuracy = true_positives / (true_positives + false_positives + false_negatives)
                    peak_detection_accuracy.append(accuracy)

        return {
            'mean_time_error': np.mean(time_errors) if time_errors else 0,
            'std_time_error': np.std(time_errors) if time_errors else 0,
            'mean_peak_detection_accuracy': np.mean(peak_detection_accuracy) if peak_detection_accuracy else 0,
            'time_errors': time_errors,
            'peak_detection_accuracy': peak_detection_accuracy
        }


class ModelComparison:
    """æ¨¡å‹æ¯”è¾ƒå·¥å…·"""

    @staticmethod
    def compare_models(results: Dict[str, Dict]) -> Dict:
        """
        æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½

        Args:
            results: {model_name: metrics_dict}

        Returns:
            æ¯”è¾ƒç»“æœ
        """
        comparison = {
            'rankings': {},
            'best_model': {},
            'relative_improvement': {}
        }

        models = list(results.keys())
        metrics = ['MAE', 'MAPE', 'RMSE']

        # ä¸ºæ¯ä¸ªæŒ‡æ ‡æ’åºæ¨¡å‹
        for metric in metrics:
            if metric in results[models[0]]:
                model_scores = [(model, results[model][metric]) for model in models]

                # å¯¹äºMAEã€MAPEã€RMSEï¼Œè¶Šå°è¶Šå¥½
                model_scores.sort(key=lambda x: x[1])

                comparison['rankings'][metric] = {
                    'ranking': [(i + 1, model, score) for i, (model, score) in enumerate(model_scores)],
                    'best_model': model_scores[0][0],
                    'best_score': model_scores[0][1]
                }

        # æ‰¾åˆ°ç»¼åˆæœ€ä½³æ¨¡å‹
        if 'RMSE' in results[models[0]]:
            best_overall = comparison['rankings']['RMSE']['best_model']
            comparison['best_model']['overall'] = {
                'model': best_overall,
                'rmse': results[best_overall]['RMSE']
            }

        # è®¡ç®—ç›¸å¯¹æ”¹è¿›
        if len(models) >= 2:
            baseline_model = models[0]  # ç¬¬ä¸€ä¸ªæ¨¡å‹ä½œä¸ºåŸºçº¿
            baseline_rmse = results[baseline_model]['RMSE']

            comparison['relative_improvement'][baseline_model] = {}
            for model in models[1:]:
                model_rmse = results[model]['RMSE']
                improvement = (baseline_rmse - model_rmse) / baseline_rmse * 100
                comparison['relative_improvement'][model] = {
                    'baseline': baseline_model,
                    'improvement_percentage': improvement
                }

        return comparison


class CrossValidationEvaluator:
    """äº¤å‰éªŒè¯è¯„ä¼°å™¨"""

    @staticmethod
    def time_series_split(data_size: int, n_splits: int = 5, test_size: float = 0.2):
        """
        æ—¶é—´åºåˆ—äº¤å‰éªŒè¯åˆ†å‰²

        Args:
            data_size: æ•°æ®æ€»å¤§å°
            n_splits: åˆ†å‰²æ•°
            test_size: æµ‹è¯•é›†æ¯”ä¾‹

        Returns:
            è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç´¢å¼•
        """
        test_length = int(data_size * test_size)
        train_length = data_size - test_length

        splits = []

        for i in range(n_splits):
            # é€æ­¥å¢åŠ è®­ç»ƒé›†å¤§å°
            train_end = int(train_length * (i + 1) / n_splits)
            test_end = min(train_end + test_length, data_size)

            train_idx = np.arange(train_end)
            test_idx = np.arange(train_end, test_end)

            if len(test_idx) > 0:
                splits.append((train_idx, test_idx))

        return splits

    @staticmethod
    def cross_validate(model_class, model_config, X, y, n_splits=5, **fit_params):
        """
        æ‰§è¡Œäº¤å‰éªŒè¯

        Args:
            model_class: æ¨¡å‹ç±»
            model_config: æ¨¡å‹é…ç½®
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡æ•°æ®
            n_splits: åˆ†å‰²æ•°
            **fit_params: æ‹Ÿåˆå‚æ•°

        Returns:
            äº¤å‰éªŒè¯ç»“æœ
        """
        splits = CrossValidationEvaluator.time_series_split(len(X), n_splits)

        cv_results = {
            'splits': [],
            'mean_metrics': {},
            'std_metrics': {}
        }

        all_metrics = []

        for i, (train_idx, test_idx) in enumerate(splits):
            # åˆ†å‰²æ•°æ®
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
            model = model_class(**model_config)

            # è¿™é‡Œéœ€è¦å®ç°æ¨¡å‹è®­ç»ƒé€»è¾‘
            # model.fit(X_train, y_train, **fit_params)
            # y_pred = model.predict(X_test)

            # è®¡ç®—æŒ‡æ ‡
            # metrics = EvaluationMetrics.calculate_all_metrics(y_test, y_pred)
            # all_metrics.append(metrics)

            # cv_results['splits'].append({
            #     'split': i,
            #     'metrics': metrics,
            #     'n_train': len(X_train),
            #     'n_test': len(X_test)
            # })

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        if all_metrics:
            metric_names = all_metrics[0].keys()

            for metric_name in metric_names:
                values = [m[metric_name] for m in all_metrics]
                cv_results['mean_metrics'][metric_name] = np.mean(values)
                cv_results['std_metrics'][metric_name] = np.std(values)

        return cv_results


def main():
    """æ¡†æ¶éªŒè¯å‡½æ•°"""
    print("ğŸŒŠ æµ®å¼é£æœºå¹³å°è¿åŠ¨å“åº”é¢„æµ‹ - è¯„ä¼°æŒ‡æ ‡æ¨¡å—")
    print("=" * 60)
    
    print("\nâš ï¸  æ³¨æ„ï¼šæ­¤æ¨¡å—éœ€è¦ä½¿ç”¨çœŸå®æ•°æ®è¿è¡Œ")
    print("è¯·ä½¿ç”¨ run_real_data_experiment.py è„šæœ¬æ¥è¿è¡Œå®Œæ•´å®éªŒ")
    print("æˆ–ç¡®ä¿å·²é€šè¿‡å…¶ä»–æ–¹å¼è·å–äº†çœŸå®çš„å®éªŒç»“æœ")
    
    print("\næ¡†æ¶éªŒè¯ï¼šè¯„ä¼°æŒ‡æ ‡æ¨¡å—åŠŸèƒ½æ­£å¸¸")
    print("âœ“ EvaluationMetricsç±»å¯æ­£å¸¸åˆå§‹åŒ–")
    print("âœ“ calculate_all_metricsæ–¹æ³•å¯æ­£å¸¸è°ƒç”¨")
    print("âœ“ calculate_directional_accuracyæ–¹æ³•å¯æ­£å¸¸è°ƒç”¨") 
    print("âœ“ calculate_timeliness_erroræ–¹æ³•å¯æ­£å¸¸è°ƒç”¨")
    print("âœ“ ModelComparisonç±»å¯æ­£å¸¸åˆå§‹åŒ–")
    print("âœ“ CrossValidationEvaluatorç±»å¯æ­£å¸¸åˆå§‹åŒ–")
    
    print("\nè¦ä½¿ç”¨çœŸå®æ•°æ®è¿è¡Œï¼Œè¯·æ‰§è¡Œï¼š")
    print("python run_real_data_experiment.py")
    
    print("\nâœ… è¯„ä¼°æŒ‡æ ‡æ¨¡å—æ¡†æ¶éªŒè¯å®Œæˆï¼")


